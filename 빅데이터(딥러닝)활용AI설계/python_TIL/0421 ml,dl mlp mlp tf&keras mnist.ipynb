{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###3 mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) tensorflow ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "y = tf.placeholder(tf.float32, [None,10])\n",
    "w = tf.Variable(tf.random_normal([784,10]))\n",
    "b = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = tf.nn.softmax(tf.matmul(x,w)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hf), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(hf,axis=1), tf.argmax(y,axis=1)), tf.float32))\n",
    "epochs = 70\n",
    "batchsize = 100\n",
    "liter = int(mnist.train.num_examples/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0001, cost : 3.001586601\n",
      "epoch : 0002, cost : 1.116221245\n",
      "epoch : 0003, cost : 0.879148518\n",
      "epoch : 0004, cost : 0.768269673\n",
      "epoch : 0005, cost : 0.697812456\n",
      "epoch : 0006, cost : 0.648908847\n",
      "epoch : 0007, cost : 0.612756636\n",
      "epoch : 0008, cost : 0.582915721\n",
      "epoch : 0009, cost : 0.557694845\n",
      "epoch : 0010, cost : 0.539050872\n",
      "epoch : 0011, cost : 0.520979222\n",
      "epoch : 0012, cost : 0.506208551\n",
      "epoch : 0013, cost : 0.494250966\n",
      "epoch : 0014, cost : 0.480590219\n",
      "epoch : 0015, cost : 0.471431103\n",
      "epoch : 0016, cost : 0.462466705\n",
      "epoch : 0017, cost : 0.451875859\n",
      "epoch : 0018, cost : 0.444990433\n",
      "epoch : 0019, cost : 0.438296721\n",
      "epoch : 0020, cost : 0.431448753\n",
      "epoch : 0021, cost : 0.422910322\n",
      "epoch : 0022, cost : 0.418789721\n",
      "epoch : 0023, cost : 0.412947948\n",
      "epoch : 0024, cost : 0.407407002\n",
      "epoch : 0025, cost : 0.403120416\n",
      "epoch : 0026, cost : 0.397831386\n",
      "epoch : 0027, cost : 0.393841302\n",
      "epoch : 0028, cost : 0.389197382\n",
      "epoch : 0029, cost : 0.386052345\n",
      "epoch : 0030, cost : 0.381789557\n",
      "epoch : 0031, cost : 0.377640482\n",
      "epoch : 0032, cost : 0.374287462\n",
      "epoch : 0033, cost : 0.371258642\n",
      "epoch : 0034, cost : 0.368376409\n",
      "epoch : 0035, cost : 0.364874066\n",
      "epoch : 0036, cost : 0.361623283\n",
      "epoch : 0037, cost : 0.359580236\n",
      "epoch : 0038, cost : 0.356823744\n",
      "epoch : 0039, cost : 0.354476045\n",
      "epoch : 0040, cost : 0.351217791\n",
      "epoch : 0041, cost : 0.349085869\n",
      "epoch : 0042, cost : 0.346741514\n",
      "epoch : 0043, cost : 0.344842033\n",
      "epoch : 0044, cost : 0.342348112\n",
      "epoch : 0045, cost : 0.340710334\n",
      "epoch : 0046, cost : 0.337783796\n",
      "epoch : 0047, cost : 0.336430076\n",
      "epoch : 0048, cost : 0.334079343\n",
      "epoch : 0049, cost : 0.332668440\n",
      "epoch : 0050, cost : 0.330937561\n",
      "epoch : 0051, cost : 0.329353987\n",
      "epoch : 0052, cost : 0.327378384\n",
      "epoch : 0053, cost : 0.325731226\n",
      "epoch : 0054, cost : 0.324204593\n",
      "epoch : 0055, cost : 0.322864180\n",
      "epoch : 0056, cost : 0.321063511\n",
      "epoch : 0057, cost : 0.320143301\n",
      "epoch : 0058, cost : 0.318891129\n",
      "epoch : 0059, cost : 0.317068937\n",
      "epoch : 0060, cost : 0.315491669\n",
      "epoch : 0061, cost : 0.314266806\n",
      "epoch : 0062, cost : 0.313071592\n",
      "epoch : 0063, cost : 0.312195201\n",
      "epoch : 0064, cost : 0.310756695\n",
      "epoch : 0065, cost : 0.310294500\n",
      "epoch : 0066, cost : 0.307872282\n",
      "epoch : 0067, cost : 0.308162735\n",
      "epoch : 0068, cost : 0.306320964\n",
      "epoch : 0069, cost : 0.304473947\n",
      "epoch : 0070, cost : 0.304758627\n",
      "정확도 :  0.9138\n",
      "label :  [7]\n",
      "pred  :  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        avg_cv = 0\n",
    "        for i in range(liter):\n",
    "            xdata, ydata = mnist.train.next_batch(batchsize)\n",
    "            _,cv = session.run([train,cost],feed_dict={x:xdata,y:ydata})\n",
    "            avg_cv += cv/liter\n",
    "        print('epoch : {:04d}, cost : {:.9f}'.format(epoch+1,avg_cv))\n",
    "    print('정확도 : ', accuracy.eval(session=session, feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "    randomnum = random.randint(0,mnist.test.num_examples-1)\n",
    "    print('label : ', session.run(tf.argmax(mnist.test.labels[randomnum:randomnum+1],1)))\n",
    "    print('pred  : ',session.run(tf.argmax(hf,1), feed_dict={x:mnist.test.images[randomnum:randomnum+1]}))\n",
    "    \n",
    "plt.imshow(mnist.test.images[randomnum:randomnum+1].reshape(28,28), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) keras ver)\n",
    "# multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain,ytrain),(xtest,ytest) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape,xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 \n",
    "xtrain = xtrain.reshape(60000,784).astype('float32')/255.0\n",
    "xtest = xtest.reshape(10000,784).astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩\n",
    "ytrain = np_utils.to_categorical(ytrain)\n",
    "ytest = np_utils.to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval = xtrain[42000:]\n",
    "xtrain = xtrain[:42000]\n",
    "\n",
    "yval = ytrain[42000:]\n",
    "ytrain = ytrain[:42000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 1s 34us/step - loss: 0.9366 - accuracy: 0.7693 - val_loss: 0.4988 - val_accuracy: 0.8699\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.4409 - accuracy: 0.8815 - val_loss: 0.3844 - val_accuracy: 0.8960\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.3670 - accuracy: 0.8978 - val_loss: 0.3418 - val_accuracy: 0.9068\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.3312 - accuracy: 0.9065 - val_loss: 0.3172 - val_accuracy: 0.9106\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 1s 31us/step - loss: 0.3082 - accuracy: 0.9129 - val_loss: 0.2991 - val_accuracy: 0.9152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ca90aa39c8>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 환경 설정\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 학습\n",
    "model.fit(xtrain, ytrain, epochs=5, batch_size=50, validation_data=(xval,yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 15us/step\n",
      "evaluation : [0.2863397601526231, 0.9203000068664551]\n"
     ]
    }
   ],
   "source": [
    "# model 평가\n",
    "\n",
    "metrics=model.evaluate(xtest, ytest, batch_size=50)\n",
    "print('evaluation : '+str(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "idx = np.random.choice(xtest.shape[0],5)\n",
    "xhat = xtest[idx]\n",
    "yhat = model.predict_classes(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 : 0실제값 : 0\n",
      "예측값 : 2실제값 : 2\n",
      "예측값 : 0실제값 : 0\n",
      "예측값 : 1실제값 : 1\n",
      "예측값 : 7실제값 : 7\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('예측값 : '+ str(yhat[i]) + '실제값 : ' +str(np.argmax(ytest[idx[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 저장\n",
    "model.save('mnist_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model architecture 확인\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "# SVG(model_to_dot(model,show_shapes=True).create(prog='dot',format='svg'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 데이터 사용\n",
    "(xtrain,ytrain),(xtest,ytest) = mnist.load_data()\n",
    "xtest = xtest.reshape(10000,784).astype('float32')/255.0\n",
    "ytest = np_utils.to_categorical(ytest)\n",
    "idx = np.random.choice(xtest.shape[0],10)\n",
    "xhat = xtest[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 불러오기\n",
    "from keras.models import load_model\n",
    "model = load_model('mnist_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 : 6실제값 : 3\n",
      "예측값 : 6실제값 : 9\n",
      "예측값 : 6실제값 : 2\n",
      "예측값 : 6실제값 : 6\n",
      "예측값 : 8실제값 : 3\n",
      "예측값 : 9실제값 : 9\n",
      "예측값 : 6실제값 : 6\n",
      "예측값 : 1실제값 : 1\n",
      "예측값 : 2실제값 : 2\n",
      "예측값 : 2실제값 : 4\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict_classes(xhat)\n",
    "\n",
    "for i in range(10):\n",
    "    print('예측값 : '+ str(yhat[i]) + '실제값 : '+str(np.argmax(ytest[idx[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = xy[:,0:-1]\n",
    "ydata = xy[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4) (8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(xdata.shape, ydata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(data):\n",
    "    bm = np.max(data,axis=0)-np.min(data,axis=0)\n",
    "    bj = data-np.min(data,axis=0)\n",
    "    return bj/bm\n",
    "    \n",
    "xy = minmax(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = xy[:,0:-1]\n",
    "ydata = xy[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[None,4])\n",
    "y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "w = tf.Variable(tf.random_normal([4,1]))\n",
    "b = tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = tf.matmul(x,w)+b\n",
    "cost = tf.reduce_mean(tf.square(hf-y))\n",
    "train = tf.train.GradientDescentOptimizer(1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost :  1.011451 \n",
      "pred :  [[0.8927503]\n",
      " [2.3938956]\n",
      " [1.6975358]\n",
      " [0.8891778]\n",
      " [1.2878156]\n",
      " [1.32742  ]\n",
      " [1.0515997]\n",
      " [1.480178 ]]\n",
      "1 cost :  1.0113984 \n",
      "pred :  [[0.8927126]\n",
      " [2.3938546]\n",
      " [1.6975019]\n",
      " [0.8891518]\n",
      " [1.287785 ]\n",
      " [1.3273902]\n",
      " [1.0515779]\n",
      " [1.4801552]]\n",
      "2 cost :  1.0113459 \n",
      "pred :  [[0.8926749]\n",
      " [2.3938136]\n",
      " [1.697468 ]\n",
      " [0.8891257]\n",
      " [1.2877544]\n",
      " [1.3273604]\n",
      " [1.0515562]\n",
      " [1.4801323]]\n",
      "3 cost :  1.011293 \n",
      "pred :  [[0.89263725]\n",
      " [2.3937726 ]\n",
      " [1.697434  ]\n",
      " [0.88909966]\n",
      " [1.2877238 ]\n",
      " [1.3273306 ]\n",
      " [1.0515344 ]\n",
      " [1.4801095 ]]\n",
      "4 cost :  1.0112404 \n",
      "pred :  [[0.8925995]\n",
      " [2.3937316]\n",
      " [1.6974001]\n",
      " [0.8890736]\n",
      " [1.287693 ]\n",
      " [1.3273007]\n",
      " [1.0515127]\n",
      " [1.4800867]]\n",
      "5 cost :  1.0111878 \n",
      "pred :  [[0.8925618]\n",
      " [2.3936906]\n",
      " [1.6973661]\n",
      " [0.8890475]\n",
      " [1.2876625]\n",
      " [1.3272709]\n",
      " [1.051491 ]\n",
      " [1.4800639]]\n",
      "6 cost :  1.0111351 \n",
      "pred :  [[0.8925241]\n",
      " [2.3936496]\n",
      " [1.6973321]\n",
      " [0.8890215]\n",
      " [1.2876318]\n",
      " [1.327241 ]\n",
      " [1.0514692]\n",
      " [1.480041 ]]\n",
      "7 cost :  1.0110824 \n",
      "pred :  [[0.89248645]\n",
      " [2.3936086 ]\n",
      " [1.6972982 ]\n",
      " [0.88899547]\n",
      " [1.2876012 ]\n",
      " [1.3272111 ]\n",
      " [1.0514475 ]\n",
      " [1.4800181 ]]\n",
      "8 cost :  1.0110297 \n",
      "pred :  [[0.8924487]\n",
      " [2.3935676]\n",
      " [1.6972642]\n",
      " [0.8889694]\n",
      " [1.2875705]\n",
      " [1.3271812]\n",
      " [1.0514257]\n",
      " [1.4799954]]\n",
      "9 cost :  1.0109771 \n",
      "pred :  [[0.892411 ]\n",
      " [2.3935266]\n",
      " [1.6972303]\n",
      " [0.8889433]\n",
      " [1.28754  ]\n",
      " [1.3271514]\n",
      " [1.051404 ]\n",
      " [1.4799726]]\n",
      "10 cost :  1.0109245 \n",
      "pred :  [[0.8923733]\n",
      " [2.3934855]\n",
      " [1.6971962]\n",
      " [0.8889173]\n",
      " [1.2875093]\n",
      " [1.3271215]\n",
      " [1.0513823]\n",
      " [1.4799497]]\n",
      "11 cost :  1.0108719 \n",
      "pred :  [[0.89233565]\n",
      " [2.3934445 ]\n",
      " [1.6971624 ]\n",
      " [0.8888912 ]\n",
      " [1.2874787 ]\n",
      " [1.3270917 ]\n",
      " [1.0513605 ]\n",
      " [1.4799268 ]]\n",
      "12 cost :  1.0108192 \n",
      "pred :  [[0.8922979]\n",
      " [2.3934035]\n",
      " [1.6971283]\n",
      " [0.8888652]\n",
      " [1.2874479]\n",
      " [1.3270619]\n",
      " [1.0513388]\n",
      " [1.479904 ]]\n",
      "13 cost :  1.0107666 \n",
      "pred :  [[0.8922602]\n",
      " [2.3933625]\n",
      " [1.6970944]\n",
      " [0.8888391]\n",
      " [1.2874174]\n",
      " [1.3270321]\n",
      " [1.051317 ]\n",
      " [1.4798813]]\n",
      "14 cost :  1.0107138 \n",
      "pred :  [[0.8922225]\n",
      " [2.3933215]\n",
      " [1.6970603]\n",
      " [0.8888131]\n",
      " [1.2873867]\n",
      " [1.327002 ]\n",
      " [1.0512953]\n",
      " [1.4798584]]\n",
      "15 cost :  1.0106612 \n",
      "pred :  [[0.89218485]\n",
      " [2.3932805 ]\n",
      " [1.6970265 ]\n",
      " [0.88878703]\n",
      " [1.2873561 ]\n",
      " [1.3269722 ]\n",
      " [1.0512736 ]\n",
      " [1.4798355 ]]\n",
      "16 cost :  1.0106084 \n",
      "pred :  [[0.8921471]\n",
      " [2.3932395]\n",
      " [1.6969924]\n",
      " [0.888761 ]\n",
      " [1.2873254]\n",
      " [1.3269424]\n",
      " [1.0512518]\n",
      " [1.4798126]]\n",
      "17 cost :  1.010556 \n",
      "pred :  [[0.8921094]\n",
      " [2.3931985]\n",
      " [1.6969585]\n",
      " [0.8887349]\n",
      " [1.2872949]\n",
      " [1.3269126]\n",
      " [1.0512301]\n",
      " [1.4797897]]\n",
      "18 cost :  1.0105033 \n",
      "pred :  [[0.8920717 ]\n",
      " [2.3931575 ]\n",
      " [1.6969247 ]\n",
      " [0.88870883]\n",
      " [1.2872641 ]\n",
      " [1.3268828 ]\n",
      " [1.0512083 ]\n",
      " [1.479767  ]]\n",
      "19 cost :  1.0104507 \n",
      "pred :  [[0.89203405]\n",
      " [2.3931165 ]\n",
      " [1.6968906 ]\n",
      " [0.88868284]\n",
      " [1.2872336 ]\n",
      " [1.3268529 ]\n",
      " [1.0511866 ]\n",
      " [1.4797442 ]]\n",
      "20 cost :  1.010398 \n",
      "pred :  [[0.8919963 ]\n",
      " [2.3930755 ]\n",
      " [1.6968567 ]\n",
      " [0.88865674]\n",
      " [1.2872028 ]\n",
      " [1.326823  ]\n",
      " [1.0511649 ]\n",
      " [1.4797213 ]]\n",
      "21 cost :  1.0103455 \n",
      "pred :  [[0.8919586]\n",
      " [2.3930345]\n",
      " [1.6968226]\n",
      " [0.8886307]\n",
      " [1.2871722]\n",
      " [1.3267932]\n",
      " [1.051143 ]\n",
      " [1.4796984]]\n",
      "22 cost :  1.0102929 \n",
      "pred :  [[0.8919209 ]\n",
      " [2.3929935 ]\n",
      " [1.6967888 ]\n",
      " [0.88860464]\n",
      " [1.2871416 ]\n",
      " [1.3267634 ]\n",
      " [1.0511214 ]\n",
      " [1.4796757 ]]\n",
      "23 cost :  1.0102401 \n",
      "pred :  [[0.89188325]\n",
      " [2.3929524 ]\n",
      " [1.6967547 ]\n",
      " [0.8885786 ]\n",
      " [1.287111  ]\n",
      " [1.3267336 ]\n",
      " [1.0510995 ]\n",
      " [1.4796529 ]]\n",
      "24 cost :  1.0101876 \n",
      "pred :  [[0.8918456]\n",
      " [2.3929114]\n",
      " [1.6967208]\n",
      " [0.8885526]\n",
      " [1.2870805]\n",
      " [1.3267038]\n",
      " [1.0510778]\n",
      " [1.47963  ]]\n",
      "25 cost :  1.010135 \n",
      "pred :  [[0.8918079]\n",
      " [2.3928704]\n",
      " [1.6966869]\n",
      " [0.8885266]\n",
      " [1.2870499]\n",
      " [1.326674 ]\n",
      " [1.0510563]\n",
      " [1.4796073]]\n",
      "26 cost :  1.0100826 \n",
      "pred :  [[0.8917703 ]\n",
      " [2.3928297 ]\n",
      " [1.696653  ]\n",
      " [0.88850063]\n",
      " [1.2870193 ]\n",
      " [1.3266442 ]\n",
      " [1.0510345 ]\n",
      " [1.4795845 ]]\n",
      "27 cost :  1.01003 \n",
      "pred :  [[0.8917327]\n",
      " [2.3927884]\n",
      " [1.6966192]\n",
      " [0.8884746]\n",
      " [1.2869887]\n",
      " [1.3266144]\n",
      " [1.0510129]\n",
      " [1.4795618]]\n",
      "28 cost :  1.0099776 \n",
      "pred :  [[0.891695 ]\n",
      " [2.3927479]\n",
      " [1.6965852]\n",
      " [0.8884486]\n",
      " [1.2869581]\n",
      " [1.3265846]\n",
      " [1.0509912]\n",
      " [1.4795389]]\n",
      "29 cost :  1.0099251 \n",
      "pred :  [[0.89165735]\n",
      " [2.3927069 ]\n",
      " [1.6965512 ]\n",
      " [0.8884226 ]\n",
      " [1.2869275 ]\n",
      " [1.3265548 ]\n",
      " [1.0509695 ]\n",
      " [1.4795163 ]]\n",
      "30 cost :  1.0098726 \n",
      "pred :  [[0.89161974]\n",
      " [2.3926659 ]\n",
      " [1.6965173 ]\n",
      " [0.8883967 ]\n",
      " [1.286897  ]\n",
      " [1.326525  ]\n",
      " [1.0509478 ]\n",
      " [1.4794934 ]]\n",
      "31 cost :  1.0098202 \n",
      "pred :  [[0.89158213]\n",
      " [2.3926249 ]\n",
      " [1.6964835 ]\n",
      " [0.88837063]\n",
      " [1.2868664 ]\n",
      " [1.3264953 ]\n",
      " [1.0509261 ]\n",
      " [1.4794707 ]]\n",
      "32 cost :  1.0097675 \n",
      "pred :  [[0.89154446]\n",
      " [2.3925838 ]\n",
      " [1.6964495 ]\n",
      " [0.88834465]\n",
      " [1.2868358 ]\n",
      " [1.3264654 ]\n",
      " [1.0509045 ]\n",
      " [1.4794478 ]]\n",
      "33 cost :  1.0097151 \n",
      "pred :  [[0.8915068 ]\n",
      " [2.3925428 ]\n",
      " [1.6964157 ]\n",
      " [0.88831866]\n",
      " [1.2868053 ]\n",
      " [1.3264356 ]\n",
      " [1.0508828 ]\n",
      " [1.4794251 ]]\n",
      "34 cost :  1.0096625 \n",
      "pred :  [[0.8914692]\n",
      " [2.3925018]\n",
      " [1.6963818]\n",
      " [0.8882927]\n",
      " [1.2867746]\n",
      " [1.3264059]\n",
      " [1.0508611]\n",
      " [1.4794023]]\n",
      "35 cost :  1.00961 \n",
      "pred :  [[0.89143157]\n",
      " [2.392461  ]\n",
      " [1.6963478 ]\n",
      " [0.8882667 ]\n",
      " [1.2867441 ]\n",
      " [1.3263761 ]\n",
      " [1.0508394 ]\n",
      " [1.4793795 ]]\n",
      "36 cost :  1.0095575 \n",
      "pred :  [[0.8913939]\n",
      " [2.39242  ]\n",
      " [1.696314 ]\n",
      " [0.8882407]\n",
      " [1.2867135]\n",
      " [1.3263463]\n",
      " [1.0508177]\n",
      " [1.4793568]]\n",
      "37 cost :  1.0095052 \n",
      "pred :  [[0.8913562 ]\n",
      " [2.3923793 ]\n",
      " [1.69628   ]\n",
      " [0.88821465]\n",
      " [1.286683  ]\n",
      " [1.3263166 ]\n",
      " [1.050796  ]\n",
      " [1.479334  ]]\n",
      "38 cost :  1.0094526 \n",
      "pred :  [[0.8913186 ]\n",
      " [2.3923383 ]\n",
      " [1.696246  ]\n",
      " [0.88818866]\n",
      " [1.2866523 ]\n",
      " [1.3262868 ]\n",
      " [1.0507743 ]\n",
      " [1.4793112 ]]\n",
      "39 cost :  1.0094001 \n",
      "pred :  [[0.891281 ]\n",
      " [2.3922973]\n",
      " [1.6962123]\n",
      " [0.8881627]\n",
      " [1.2866217]\n",
      " [1.326257 ]\n",
      " [1.0507526]\n",
      " [1.4792885]]\n",
      "40 cost :  1.0093477 \n",
      "pred :  [[0.89124334]\n",
      " [2.3922563 ]\n",
      " [1.6961784 ]\n",
      " [0.8881367 ]\n",
      " [1.2865912 ]\n",
      " [1.3262272 ]\n",
      " [1.050731  ]\n",
      " [1.4792657 ]]\n",
      "41 cost :  1.009295 \n",
      "pred :  [[0.89120567]\n",
      " [2.3922153 ]\n",
      " [1.6961445 ]\n",
      " [0.8881107 ]\n",
      " [1.2865605 ]\n",
      " [1.3261974 ]\n",
      " [1.0507092 ]\n",
      " [1.4792429 ]]\n",
      "42 cost :  1.0092427 \n",
      "pred :  [[0.8911681]\n",
      " [2.3921745]\n",
      " [1.6961106]\n",
      " [0.8880847]\n",
      " [1.2865301]\n",
      " [1.3261676]\n",
      " [1.0506876]\n",
      " [1.4792202]]\n",
      "43 cost :  1.00919 \n",
      "pred :  [[0.89113045]\n",
      " [2.3921332 ]\n",
      " [1.6960765 ]\n",
      " [0.8880587 ]\n",
      " [1.2864995 ]\n",
      " [1.3261378 ]\n",
      " [1.0506659 ]\n",
      " [1.4791974 ]]\n",
      "44 cost :  1.0091377 \n",
      "pred :  [[0.8910929 ]\n",
      " [2.3920927 ]\n",
      " [1.6960428 ]\n",
      " [0.88803273]\n",
      " [1.2864689 ]\n",
      " [1.326108  ]\n",
      " [1.0506442 ]\n",
      " [1.4791746 ]]\n",
      "45 cost :  1.0090853 \n",
      "pred :  [[0.8910552]\n",
      " [2.3920517]\n",
      " [1.6960089]\n",
      " [0.8880068]\n",
      " [1.2864385]\n",
      " [1.3260783]\n",
      " [1.0506226]\n",
      " [1.4791518]]\n",
      "46 cost :  1.0090327 \n",
      "pred :  [[0.8910177]\n",
      " [2.3920107]\n",
      " [1.6959751]\n",
      " [0.8879808]\n",
      " [1.2864078]\n",
      " [1.3260485]\n",
      " [1.0506009]\n",
      " [1.4791291]]\n",
      "47 cost :  1.0089803 \n",
      "pred :  [[0.89098   ]\n",
      " [2.3919697 ]\n",
      " [1.6959411 ]\n",
      " [0.88795483]\n",
      " [1.2863773 ]\n",
      " [1.3260187 ]\n",
      " [1.0505792 ]\n",
      " [1.4791063 ]]\n",
      "48 cost :  1.0089278 \n",
      "pred :  [[0.89094245]\n",
      " [2.3919287 ]\n",
      " [1.6959072 ]\n",
      " [0.88792884]\n",
      " [1.2863467 ]\n",
      " [1.325989  ]\n",
      " [1.0505576 ]\n",
      " [1.4790835 ]]\n",
      "49 cost :  1.0088754 \n",
      "pred :  [[0.8909048 ]\n",
      " [2.391888  ]\n",
      " [1.6958734 ]\n",
      " [0.88790286]\n",
      " [1.2863162 ]\n",
      " [1.3259592 ]\n",
      " [1.0505359 ]\n",
      " [1.4790608 ]]\n",
      "50 cost :  1.0088229 \n",
      "pred :  [[0.89086723]\n",
      " [2.391847  ]\n",
      " [1.6958395 ]\n",
      " [0.88787687]\n",
      " [1.2862855 ]\n",
      " [1.3259294 ]\n",
      " [1.0505142 ]\n",
      " [1.479038  ]]\n",
      "51 cost :  1.0087705 \n",
      "pred :  [[0.89082956]\n",
      " [2.3918061 ]\n",
      " [1.6958054 ]\n",
      " [0.8878509 ]\n",
      " [1.286255  ]\n",
      " [1.3258996 ]\n",
      " [1.0504925 ]\n",
      " [1.4790151 ]]\n",
      "52 cost :  1.008718 \n",
      "pred :  [[0.890792 ]\n",
      " [2.391765 ]\n",
      " [1.6957717]\n",
      " [0.8878249]\n",
      " [1.2862244]\n",
      " [1.3258698]\n",
      " [1.0504708]\n",
      " [1.4789925]]\n",
      "53 cost :  1.0086656 \n",
      "pred :  [[0.89075434]\n",
      " [2.391724  ]\n",
      " [1.6957377 ]\n",
      " [0.8877989 ]\n",
      " [1.2861938 ]\n",
      " [1.32584   ]\n",
      " [1.0504491 ]\n",
      " [1.4789696 ]]\n",
      "54 cost :  1.0086131 \n",
      "pred :  [[0.8907168]\n",
      " [2.391683 ]\n",
      " [1.6957039]\n",
      " [0.8877729]\n",
      " [1.2861633]\n",
      " [1.3258102]\n",
      " [1.0504274]\n",
      " [1.4789469]]\n",
      "55 cost :  1.0085607 \n",
      "pred :  [[0.8906791]\n",
      " [2.391642 ]\n",
      " [1.69567  ]\n",
      " [0.887747 ]\n",
      " [1.2861328]\n",
      " [1.3257804]\n",
      " [1.0504057]\n",
      " [1.478924 ]]\n",
      "56 cost :  1.0085084 \n",
      "pred :  [[0.89064157]\n",
      " [2.3916016 ]\n",
      " [1.6956362 ]\n",
      " [0.88772094]\n",
      " [1.2861022 ]\n",
      " [1.3257507 ]\n",
      " [1.0503842 ]\n",
      " [1.4789014 ]]\n",
      "57 cost :  1.0084559 \n",
      "pred :  [[0.8906039]\n",
      " [2.3915606]\n",
      " [1.6956023]\n",
      " [0.887695 ]\n",
      " [1.2860715]\n",
      " [1.3257209]\n",
      " [1.0503625]\n",
      " [1.4788785]]\n",
      "58 cost :  1.0084035 \n",
      "pred :  [[0.89056635]\n",
      " [2.3915195 ]\n",
      " [1.6955683 ]\n",
      " [0.887669  ]\n",
      " [1.286041  ]\n",
      " [1.3256912 ]\n",
      " [1.0503408 ]\n",
      " [1.4788558 ]]\n",
      "59 cost :  1.0083508 \n",
      "pred :  [[0.8905287 ]\n",
      " [2.3914785 ]\n",
      " [1.6955345 ]\n",
      " [0.88764304]\n",
      " [1.2860105 ]\n",
      " [1.3256614 ]\n",
      " [1.0503191 ]\n",
      " [1.478833  ]]\n",
      "60 cost :  1.0082985 \n",
      "pred :  [[0.8904911 ]\n",
      " [2.3914375 ]\n",
      " [1.6955005 ]\n",
      " [0.88761705]\n",
      " [1.28598   ]\n",
      " [1.3256316 ]\n",
      " [1.0502974 ]\n",
      " [1.4788103 ]]\n",
      "61 cost :  1.0082462 \n",
      "pred :  [[0.89045346]\n",
      " [2.3913968 ]\n",
      " [1.6954666 ]\n",
      " [0.88759106]\n",
      " [1.2859493 ]\n",
      " [1.3256018 ]\n",
      " [1.0502758 ]\n",
      " [1.4787874 ]]\n",
      "62 cost :  1.0081937 \n",
      "pred :  [[0.8904159]\n",
      " [2.3913558]\n",
      " [1.6954328]\n",
      " [0.8875651]\n",
      " [1.2859188]\n",
      " [1.3255721]\n",
      " [1.0502541]\n",
      " [1.4787647]]\n",
      "63 cost :  1.0081413 \n",
      "pred :  [[0.89037824]\n",
      " [2.391315  ]\n",
      " [1.6953989 ]\n",
      " [0.88753915]\n",
      " [1.2858882 ]\n",
      " [1.3255422 ]\n",
      " [1.0502324 ]\n",
      " [1.4787419 ]]\n",
      "64 cost :  1.0080888 \n",
      "pred :  [[0.8903407]\n",
      " [2.391274 ]\n",
      " [1.695365 ]\n",
      " [0.8875131]\n",
      " [1.2858577]\n",
      " [1.3255125]\n",
      " [1.0502107]\n",
      " [1.4787191]]\n",
      "65 cost :  1.0080364 \n",
      "pred :  [[0.890303 ]\n",
      " [2.391233 ]\n",
      " [1.6953311]\n",
      " [0.8874872]\n",
      " [1.2858272]\n",
      " [1.3254827]\n",
      " [1.050189 ]\n",
      " [1.4786963]]\n",
      "66 cost :  1.007984 \n",
      "pred :  [[0.89026546]\n",
      " [2.391192  ]\n",
      " [1.6952972 ]\n",
      " [0.8874612 ]\n",
      " [1.2857966 ]\n",
      " [1.325453  ]\n",
      " [1.0501673 ]\n",
      " [1.4786736 ]]\n",
      "67 cost :  1.0079315 \n",
      "pred :  [[0.8902278]\n",
      " [2.391151 ]\n",
      " [1.6952633]\n",
      " [0.8874352]\n",
      " [1.285766 ]\n",
      " [1.3254231]\n",
      " [1.0501456]\n",
      " [1.4786508]]\n",
      "68 cost :  1.0078793 \n",
      "pred :  [[0.89019024]\n",
      " [2.3911104 ]\n",
      " [1.6952294 ]\n",
      " [0.8874092 ]\n",
      " [1.2857355 ]\n",
      " [1.3253934 ]\n",
      " [1.0501239 ]\n",
      " [1.478628  ]]\n",
      "69 cost :  1.0078268 \n",
      "pred :  [[0.8901526]\n",
      " [2.3910692]\n",
      " [1.6951956]\n",
      " [0.8873832]\n",
      " [1.2857049]\n",
      " [1.3253636]\n",
      " [1.0501024]\n",
      " [1.4786053]]\n",
      "70 cost :  1.0077744 \n",
      "pred :  [[0.890115  ]\n",
      " [2.3910284 ]\n",
      " [1.6951617 ]\n",
      " [0.88735723]\n",
      " [1.2856743 ]\n",
      " [1.3253338 ]\n",
      " [1.0500807 ]\n",
      " [1.4785824 ]]\n",
      "71 cost :  1.0077219 \n",
      "pred :  [[0.89007735]\n",
      " [2.3909874 ]\n",
      " [1.6951277 ]\n",
      " [0.88733125]\n",
      " [1.2856437 ]\n",
      " [1.325304  ]\n",
      " [1.050059  ]\n",
      " [1.4785597 ]]\n",
      "72 cost :  1.0076694 \n",
      "pred :  [[0.8900398 ]\n",
      " [2.3909464 ]\n",
      " [1.6950939 ]\n",
      " [0.88730526]\n",
      " [1.2856132 ]\n",
      " [1.3252743 ]\n",
      " [1.0500373 ]\n",
      " [1.4785368 ]]\n",
      "73 cost :  1.0076171 \n",
      "pred :  [[0.89000213]\n",
      " [2.3909054 ]\n",
      " [1.6950601 ]\n",
      " [0.88727933]\n",
      " [1.2855827 ]\n",
      " [1.3252445 ]\n",
      " [1.0500157 ]\n",
      " [1.4785142 ]]\n",
      "74 cost :  1.0075645 \n",
      "pred :  [[0.8899646]\n",
      " [2.3908644]\n",
      " [1.6950262]\n",
      " [0.8872533]\n",
      " [1.285552 ]\n",
      " [1.3252147]\n",
      " [1.049994 ]\n",
      " [1.4784913]]\n",
      "75 cost :  1.0075125 \n",
      "pred :  [[0.8899269 ]\n",
      " [2.3908238 ]\n",
      " [1.6949923 ]\n",
      " [0.88722736]\n",
      " [1.2855215 ]\n",
      " [1.3251851 ]\n",
      " [1.0499723 ]\n",
      " [1.4784687 ]]\n",
      "76 cost :  1.0074599 \n",
      "pred :  [[0.88988936]\n",
      " [2.3907828 ]\n",
      " [1.6949583 ]\n",
      " [0.88720137]\n",
      " [1.285491  ]\n",
      " [1.3251553 ]\n",
      " [1.0499506 ]\n",
      " [1.4784458 ]]\n",
      "77 cost :  1.0074075 \n",
      "pred :  [[0.8898517 ]\n",
      " [2.3907418 ]\n",
      " [1.6949245 ]\n",
      " [0.88717544]\n",
      " [1.2854604 ]\n",
      " [1.3251255 ]\n",
      " [1.0499289 ]\n",
      " [1.4784231 ]]\n",
      "78 cost :  1.0073551 \n",
      "pred :  [[0.88981414]\n",
      " [2.3907008 ]\n",
      " [1.6948905 ]\n",
      " [0.88714945]\n",
      " [1.2854298 ]\n",
      " [1.3250957 ]\n",
      " [1.0499072 ]\n",
      " [1.4784002 ]]\n",
      "79 cost :  1.0073028 \n",
      "pred :  [[0.88977647]\n",
      " [2.39066   ]\n",
      " [1.6948568 ]\n",
      " [0.8871234 ]\n",
      " [1.2853992 ]\n",
      " [1.3250659 ]\n",
      " [1.0498855 ]\n",
      " [1.4783776 ]]\n",
      "80 cost :  1.0072503 \n",
      "pred :  [[0.8897389]\n",
      " [2.390619 ]\n",
      " [1.6948229]\n",
      " [0.8870975]\n",
      " [1.2853687]\n",
      " [1.3250362]\n",
      " [1.0498638]\n",
      " [1.4783547]]\n",
      "81 cost :  1.007198 \n",
      "pred :  [[0.88970125]\n",
      " [2.390578  ]\n",
      " [1.6947889 ]\n",
      " [0.88707143]\n",
      " [1.2853382 ]\n",
      " [1.3250064 ]\n",
      " [1.0498421 ]\n",
      " [1.478332  ]]\n",
      "82 cost :  1.0071456 \n",
      "pred :  [[0.8896637]\n",
      " [2.3905373]\n",
      " [1.694755 ]\n",
      " [0.8870455]\n",
      " [1.2853076]\n",
      " [1.3249766]\n",
      " [1.0498205]\n",
      " [1.4783092]]\n",
      "83 cost :  1.0070933 \n",
      "pred :  [[0.889626 ]\n",
      " [2.3904963]\n",
      " [1.6947211]\n",
      " [0.8870195]\n",
      " [1.2852771]\n",
      " [1.3249469]\n",
      " [1.0497988]\n",
      " [1.4782865]]\n",
      "84 cost :  1.0070407 \n",
      "pred :  [[0.8895885]\n",
      " [2.3904552]\n",
      " [1.6946872]\n",
      " [0.8869935]\n",
      " [1.2852465]\n",
      " [1.3249171]\n",
      " [1.0497772]\n",
      " [1.4782636]]\n",
      "85 cost :  1.0069883 \n",
      "pred :  [[0.8895508 ]\n",
      " [2.3904142 ]\n",
      " [1.6946534 ]\n",
      " [0.88696754]\n",
      " [1.2852159 ]\n",
      " [1.3248873 ]\n",
      " [1.0497555 ]\n",
      " [1.4782408 ]]\n",
      "86 cost :  1.0069361 \n",
      "pred :  [[0.88951325]\n",
      " [2.3903737 ]\n",
      " [1.6946194 ]\n",
      " [0.88694155]\n",
      " [1.2851853 ]\n",
      " [1.3248575 ]\n",
      " [1.0497339 ]\n",
      " [1.4782181 ]]\n",
      "87 cost :  1.0068836 \n",
      "pred :  [[0.8894756 ]\n",
      " [2.3903327 ]\n",
      " [1.6945857 ]\n",
      " [0.88691556]\n",
      " [1.2851548 ]\n",
      " [1.3248277 ]\n",
      " [1.0497122 ]\n",
      " [1.4781953 ]]\n",
      "88 cost :  1.0068312 \n",
      "pred :  [[0.88943803]\n",
      " [2.3902915 ]\n",
      " [1.6945517 ]\n",
      " [0.8868896 ]\n",
      " [1.2851243 ]\n",
      " [1.3247979 ]\n",
      " [1.0496905 ]\n",
      " [1.4781725 ]]\n",
      "89 cost :  1.0067788 \n",
      "pred :  [[0.88940036]\n",
      " [2.3902507 ]\n",
      " [1.6945179 ]\n",
      " [0.88686365]\n",
      " [1.2850937 ]\n",
      " [1.3247681 ]\n",
      " [1.0496688 ]\n",
      " [1.4781498 ]]\n",
      "90 cost :  1.0067265 \n",
      "pred :  [[0.8893628 ]\n",
      " [2.3902097 ]\n",
      " [1.6944839 ]\n",
      " [0.88683766]\n",
      " [1.285063  ]\n",
      " [1.3247385 ]\n",
      " [1.0496471 ]\n",
      " [1.478127  ]]\n",
      "91 cost :  1.0066742 \n",
      "pred :  [[0.88932514]\n",
      " [2.390169  ]\n",
      " [1.69445   ]\n",
      " [0.8868116 ]\n",
      " [1.2850325 ]\n",
      " [1.3247086 ]\n",
      " [1.0496254 ]\n",
      " [1.4781042 ]]\n",
      "92 cost :  1.0066217 \n",
      "pred :  [[0.8892876]\n",
      " [2.3901277]\n",
      " [1.6944162]\n",
      " [0.8867857]\n",
      " [1.285002 ]\n",
      " [1.3246788]\n",
      " [1.0496037]\n",
      " [1.4780815]]\n",
      "93 cost :  1.0065695 \n",
      "pred :  [[0.8892499]\n",
      " [2.3900871]\n",
      " [1.6943823]\n",
      " [0.8867597]\n",
      " [1.2849715]\n",
      " [1.3246491]\n",
      " [1.049582 ]\n",
      " [1.4780587]]\n",
      "94 cost :  1.0065172 \n",
      "pred :  [[0.88921237]\n",
      " [2.3900461 ]\n",
      " [1.6943485 ]\n",
      " [0.8867337 ]\n",
      " [1.284941  ]\n",
      " [1.3246193 ]\n",
      " [1.0495603 ]\n",
      " [1.4780359 ]]\n",
      "95 cost :  1.0064647 \n",
      "pred :  [[0.8891747]\n",
      " [2.390005 ]\n",
      " [1.6943145]\n",
      " [0.8867077]\n",
      " [1.2849103]\n",
      " [1.3245895]\n",
      " [1.0495387]\n",
      " [1.4780132]]\n",
      "96 cost :  1.0064124 \n",
      "pred :  [[0.88913715]\n",
      " [2.389964  ]\n",
      " [1.6942806 ]\n",
      " [0.88668174]\n",
      " [1.2848798 ]\n",
      " [1.3245598 ]\n",
      " [1.049517  ]\n",
      " [1.4779904 ]]\n",
      "97 cost :  1.0063599 \n",
      "pred :  [[0.8890995 ]\n",
      " [2.389923  ]\n",
      " [1.6942466 ]\n",
      " [0.88665575]\n",
      " [1.2848492 ]\n",
      " [1.3245299 ]\n",
      " [1.0494953 ]\n",
      " [1.4779676 ]]\n",
      "98 cost :  1.0063076 \n",
      "pred :  [[0.8890619]\n",
      " [2.3898823]\n",
      " [1.6942128]\n",
      " [0.8866298]\n",
      " [1.2848186]\n",
      " [1.3245001]\n",
      " [1.0494736]\n",
      " [1.4779449]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 cost :  1.0062553 \n",
      "pred :  [[0.88902426]\n",
      " [2.3898413 ]\n",
      " [1.6941789 ]\n",
      " [0.8866038 ]\n",
      " [1.2847881 ]\n",
      " [1.3244704 ]\n",
      " [1.0494521 ]\n",
      " [1.4779221 ]]\n",
      "100 cost :  1.0062029 \n",
      "pred :  [[0.8889867 ]\n",
      " [2.3898005 ]\n",
      " [1.6941451 ]\n",
      " [0.88657784]\n",
      " [1.2847575 ]\n",
      " [1.3244407 ]\n",
      " [1.0494304 ]\n",
      " [1.4778993 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for step in range(101):\n",
    "        cv,hv,_ = session.run([cost,hf,train],feed_dict={x:xdata,y:ydata})\n",
    "        print(step, 'cost : ',cv, '\\npred : ', hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xor 문제 구현\n",
    "# tf 단일 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]])\n",
    "ydata = np.array([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "print(xdata.shape, ydata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "w = tf.Variable(tf.random_normal([2,1]))\n",
    "b = tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = tf.sigmoid(tf.matmul(x,w)+b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.cast(hf > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7012743\n",
      "1000 0.69314766\n",
      "2000 0.6931472\n",
      "3000 0.6931472\n",
      "4000 0.6931472\n",
      "5000 0.6931472\n",
      "6000 0.6931472\n",
      "7000 0.6931472\n",
      "8000 0.6931472\n",
      "9000 0.6931472\n",
      "10000 0.6931472\n",
      "\n",
      "Hypothesis :  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Predicted :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy :  0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cv = session.run([train,cost], feed_dict={x:xdata, y:ydata})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cv)\n",
    "\n",
    "    hv, pv, av = session.run([hf,pred,accuracy], feed_dict={x:xdata, y:ydata})\n",
    "    print(\"\\nHypothesis : \", hv, \"\\nPredicted : \", pv, \"\\nAccuracy : \", av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xor 문제 구현\n",
    "# tf 멀티 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]])\n",
    "ydata = np.array([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,2]))\n",
    "b1 = tf.Variable(tf.random_normal([2]))\n",
    "layer1 = tf.sigmoid(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([2,2]))\n",
    "b2 = tf.Variable(tf.random_normal([2]))\n",
    "layer2 = tf.sigmoid(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w3 = tf.Variable(tf.random_normal([2,1]))\n",
    "b3 = tf.Variable(tf.random_normal([1]))\n",
    "hf = tf.sigmoid(tf.matmul(layer1,w3)+b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.cast(hf > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.758927\n",
      "1000 0.692829\n",
      "2000 0.6805557\n",
      "3000 0.5333448\n",
      "4000 0.41674107\n",
      "5000 0.38378555\n",
      "6000 0.37094587\n",
      "7000 0.3644222\n",
      "8000 0.3605461\n",
      "9000 0.3580016\n",
      "10000 0.35621285\n",
      "\n",
      "Hypothesis :  [[0.01091694]\n",
      " [0.49586132]\n",
      " [0.98909235]\n",
      " [0.5041306 ]] \n",
      "Predicted :  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy :  0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cv = session.run([train,cost], feed_dict={x:xdata, y:ydata})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cv)\n",
    "\n",
    "    hv, pv, av = session.run([hf,pred,accuracy], feed_dict={x:xdata, y:ydata})\n",
    "    print(\"\\nHypothesis : \", hv, \"\\nPredicted : \", pv, \"\\nAccuracy : \", av)\n",
    "    #0.01091694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis :  [[5.7107210e-04 1.1042356e-03 8.0111623e-04 1.3730824e-03 4.5457482e-04\n",
      "  7.5134635e-04 5.5369735e-04 1.3462603e-03 1.4874339e-03 8.9237094e-04]\n",
      " [9.9808878e-01 9.9901116e-01 9.9863708e-01 9.9920487e-01 9.9759960e-01\n",
      "  9.9854684e-01 9.9802887e-01 9.9918890e-01 9.9926597e-01 9.9877644e-01]\n",
      " [9.9791783e-01 9.9892271e-01 9.9851513e-01 9.9913359e-01 9.9738497e-01\n",
      "  9.9841690e-01 9.9785256e-01 9.9911630e-01 9.9920022e-01 9.9866688e-01]\n",
      " [1.3376474e-03 2.5847256e-03 1.8757880e-03 3.2125115e-03 1.0648370e-03\n",
      "  1.7594695e-03 1.2969971e-03 3.1500161e-03 3.4796596e-03 2.0892918e-03]] \n",
      "Predicted :  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]])\n",
    "ydata = np.array([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "w1= tf.Variable(tf.random_normal([2,10]))\n",
    "b1 = tf.Variable(tf.random_normal([10]))\n",
    "layer1 = tf.sigmoid(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2= tf.Variable(tf.random_normal([10,10]))\n",
    "b2 = tf.Variable(tf.random_normal([10]))\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,w2)+b2)\n",
    "\n",
    "w3= tf.Variable(tf.random_normal([10,10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "layer4 = tf.sigmoid(tf.matmul(layer2,w3)+b3)\n",
    "\n",
    "w4= tf.Variable(tf.random_normal([10,1]))\n",
    "b4 = tf.Variable(tf.random_normal([1]))\n",
    "hf = tf.sigmoid(tf.matmul(layer4,w4)+b1)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hf > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cv = session.run([train,cost], feed_dict={x:xdata, y:ydata})\n",
    "#         if step % 1000 == 0:\n",
    "#             print(step, cv)\n",
    "\n",
    "    hv, pv, av = session.run([hf,pred,accuracy], feed_dict={x:xdata, y:ydata})\n",
    "    print(\"\\nHypothesis : \", hv, \"\\nPredicted : \", pv, \"\\nAccuracy : \", av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
